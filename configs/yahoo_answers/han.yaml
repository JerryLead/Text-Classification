# global parameters
model_name: han  # 'han', 'fasttext', 'attbilstm', 'textcnn'
                 # refer to README.md for more info about each model

# dataset parameters
dataset: yahoo_answers  # 'ag_news', 'dbpedia', 'yelp_review_polarity', 'yelp_review_full', 'yahoo_answers', 'amazon_review_polarity', 'amazon_review_full'
                  # refer to README.md for more info about each dataset
dataset_path: /Users/zou/Desktop/Text-Classification/data/yahoo_answers  # folder with dataset
output_path: /Users/zou/Desktop/Text-Classification/data/outputs/yahoo_answers/docs   # folder with data files saved by preprocess.py

# preprocess parameters
sentence_limit: 15
word_limit: 20
min_word_count: 5

# word embeddings parameters
word2vec: glove   # 'random': initialize embedding weights randomly
                  # 'glove': load Glove as pre-trained word embeddings
word2vec_folder: /Users/zou/Desktop/Text-Classification/data/glove  # only makes sense when `word2vec: glove`
word2vec_name: glove.6B.300d.txt  # only makes sense when `word2vec: glove`
emb_size: 256  # word embedding size
               # only makes sense when `word2vec: random`

# model parameters
word_rnn_size: 50  # word RNN size
sentence_rnn_size: 50  # character RNN size
word_rnn_layers: 1  # number of layers in character RNN
sentence_rnn_layers: 1  # number of layers in word RNN
word_att_size: 100  # size of the word-level attention layer (also the size of the word context vector)
sentence_att_size: 100  # size of the sentence-level attention layer (also the size of the sentence context vector)
dropout: 0.3  # dropout
fine_tune_word_embeddings: True  # fine-tune word embeddings?

# checkpoint saving parameters
checkpoint_path: /Users/zou/Desktop/Text-Classification/checkpoints  # path to save checkpoints, null if never save checkpoints
checkpoint_basename: checkpoint_han_yahoo  # basename of the checkpoint

# training parameters
start_epoch: 0  # start at this epoch
batch_size: 512  # batch size
lr: 0.001  # learning rate
lr_decay: 0.3  # a factor to multiply learning rate with (0, 1)
workers: 4  # number of workers for loading data in the DataLoader
num_epochs: 5  # number of epochs to run
grad_clip: null  # clip gradients at this value, null if never clip gradients
print_freq: 2000  # print training status every __ batches
checkpoint: null  # path to model checkpoint, null if none